{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c011c327",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f63182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous, numerical data.\n",
    "import pandas as pd\n",
    "#     [0, 0, 1],  # Red, Round, Large (Apple)\n",
    "#     [1, 1, 0],  # Yellow, Elongated, Small (Banana)\n",
    "#     [0, 0, 0],  # Red, Round, Small (Apple)\n",
    "#     [1, 1, 1]   # Yellow, Elongated, Large (Banana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b936e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color (0: Red, 1: Yellow)</th>\n",
       "      <th>Shape (0: Round, 1: Elongated)</th>\n",
       "      <th>Size (0: Small, 1: Large)</th>\n",
       "      <th>Fruit (0: Apple, 1: Banana)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Color (0: Red, 1: Yellow)  Shape (0: Round, 1: Elongated)  \\\n",
       "0                          0                               0   \n",
       "1                          1                               1   \n",
       "2                          0                               0   \n",
       "3                          1                               1   \n",
       "\n",
       "   Size (0: Small, 1: Large)  Fruit (0: Apple, 1: Banana)  \n",
       "0                          1                            0  \n",
       "1                          0                            1  \n",
       "2                          0                            0  \n",
       "3                          1                            1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fruit.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0563da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Fruit (0: Apple, 1: Banana)',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8fce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['Fruit (0: Apple, 1: Banana)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33785e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9b7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafec70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cbac59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "337674f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48acf891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516d4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data:    Color (0: Red, 1: Yellow)  Shape (0: Round, 1: Elongated)  \\\n",
      "1                          1                               1   \n",
      "\n",
      "   Size (0: Small, 1: Large)  \n",
      "1                          0  \n",
      "True Labels: 1    1\n",
      "Name: Fruit (0: Apple, 1: Banana), dtype: int64\n",
      "Predicted Labels: [1]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test Data:\", X_test)\n",
    "print(\"True Labels:\", y_test)\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee269444",
   "metadata": {},
   "source": [
    "# BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "016b50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: [[0, 0, 0]]\n",
      "True Labels: [1]\n",
      "Predicted Labels: [1]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Dataset\n",
    "# Features: [Color, Shape, Size]\n",
    "X = [\n",
    "    [1, 1, 1],  # Red, Round, Large (Apple)\n",
    "    [0, 0, 0],  # Not Red, Not Round, Not Large (Banana)\n",
    "    [1, 1, 0],  # Red, Round, Not Large (Apple)\n",
    "    [0, 0, 1]   # Not Red, Not Round, Large (Banana)\n",
    "]\n",
    "# Labels: [Fruit Type]\n",
    "y = [0, 1, 0, 1]  # 0: Apple, 1: Banana\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "model = BernoulliNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Test Data:\", X_test)\n",
    "print(\"True Labels:\", y_test)\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918a644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bd2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b490ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bbe693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4961e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "134d9543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01ccfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0441ea94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11477e4d",
   "metadata": {},
   "source": [
    "# MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e88fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: [[0, 1, 0, 3], [0, 2, 1, 1]]\n",
      "True Labels: [1, 1]\n",
      "Predicted Labels: [0 0]\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dataset\n",
    "# Features: [Buy, Win, Offer, Free]\n",
    "X = [\n",
    "    [2, 0, 1, 0],  # Not Spam: Moderate frequency of \"Buy\" and \"Offer\"\n",
    "    [0, 1, 0, 3],  # Spam: High frequency of \"Free\" and \"Win\"\n",
    "    [1, 0, 2, 0],  # Not Spam: Moderate frequency of \"Buy\" and \"Offer\"\n",
    "    [0, 2, 1, 1]   # Spam: High frequency of \"Win\", moderate \"Free\" and \"Offer\"\n",
    "]\n",
    "# Labels: [Email Type]\n",
    "y = [0, 1, 0, 1]  # 0: Not Spam, 1: Spam\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Test Data:\", X_test)\n",
    "print(\"True Labels:\", y_test)\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6b6820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ayush\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45646cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Breaking down text into smaller parts, like sentences or words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ee2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3461b89c",
   "metadata": {},
   "source": [
    "NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f58dfcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK, or Natural Language Toolkit, is a Python library that provides tools for processing and analyzing text data. \n",
    "# It's one of the most popular and powerful libraries for natural language processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1656ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aaeb415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool \n",
    "#that is specifically attuned to sentiments expressed in social media, \n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c628bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d8dcfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome to the fun session of Artificial Intelligence at Islington!.', 'But I live in Dharan']\n",
      "['Welcome', 'to', 'the', 'fun', 'session', 'of', 'Artificial', 'Intelligence', 'at', 'Islington', '!', '.', 'But', 'I', 'live', 'in', 'Dharan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = \"Welcome to the fun session of Artificial Intelligence at Islington!. But I live in Dharan\"\n",
    "print(sent_tokenize(text))  # Sentence Tokenization\n",
    "print(word_tokenize(text))  # Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4123b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus: is a large and structured set of text.\n",
    "# corpora: NLTK provides a collection of pre-loaded datasets, known as built-in corpora, for common NLP tasks.\n",
    "# The movie_reviews corpus contains movie reviews \n",
    "# review is labeled as either Positive (pos) or Negative (neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews #imports movie reviews from nltk\n",
    "from nltk.corpus import stopwords #imports stopwords from nltk\n",
    "from nltk.corpus import wordnet #imports wordnet(lexical database for the english language) from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d0082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c1806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a1d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inbuilt list of stopwords in nltk\n",
    "stopwords.words('english')[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8db59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13948f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints all words in movie_review with file id ‘....’\n",
    "movie_reviews.words('neg/cv003_12683.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40715fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \" quest for camelot \" is warner bros . \\' first feature-length , fully-animated attempt to steal clout from disney\\'s cartoon empire , but the mouse has no reason to be worried . \\nthe only other recent challenger to their throne was last fall\\'s promising , if flawed , 20th century fox production \" anastasia , \" but disney\\'s \" hercules , \" with its lively cast and colorful palate , had her beat hands-down when it came time to crown 1997\\'s best piece of animation . \\nthis year , it\\'s no contest , as \" quest for camelot \" is pretty much dead on arrival . \\neven the magic kingdom at its most mediocre -- that\\'d be \" pocahontas \" for those of you keeping score -- isn\\'t nearly as dull as this . \\nthe story revolves around the adventures of free-spirited kayley ( voiced by jessalyn gilsig ) , the early-teen daughter of a belated knight from king arthur\\'s round table . \\nkayley\\'s only dream is to follow in her father\\'s footsteps , and she gets her chance when evil warlord ruber ( gary oldman ) , an ex-round table member-gone-bad , steals arthur\\'s magical sword excalibur and accidentally loses it in a dangerous , booby-trapped forest . \\nwith the help of hunky , blind timberland-dweller garrett ( carey elwes ) and a two-headed dragon ( eric idle and don rickles ) that\\'s always arguing with itself , kayley just might be able to break the medieval sexist mold and prove her worth as a fighter on arthur\\'s side . \\n \" quest for camelot \" is missing pure showmanship , an essential element if it\\'s ever expected to climb to the high ranks of disney . \\nthere\\'s nothing here that differentiates \" quest \" from something you\\'d see on any given saturday morning cartoon -- subpar animation , instantly forgettable songs , poorly-integrated computerized footage . \\n ( compare kayley and garrett\\'s run-in with the angry ogre to herc\\'s battle with the hydra . \\ni rest my case . ) \\neven the characters stink -- none of them are remotely interesting , so much that the film becomes a race to see which one can out-bland the others . \\nin the end , it\\'s a tie -- they all win . \\nthat dragon\\'s comedy shtick is awfully cloying , but at least it shows signs of a pulse . \\nat least fans of the early-\\'90s tgif television line-up will be thrilled to find jaleel \" urkel \" white and bronson \" balki \" pinchot sharing the same footage . \\na few scenes are nicely realized ( though i\\'m at a loss to recall enough to be specific ) , and the actors providing the voice talent are enthusiastic ( though most are paired up with singers who don\\'t sound a thing like them for their big musical moments -- jane seymour and celine dion ? ? ? ) . \\nbut one must strain through too much of this mess to find the good . \\naside from the fact that children will probably be as bored watching this as adults , \" quest for camelot \" \\'s most grievous error is its complete lack of personality . \\nand personality , we learn from this mess , goes a very long way . \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.raw('neg/cv003_12683.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7105b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e10a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df6442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406c66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Congrats, You have won!! reply to our sms for a free nokia mobile + free camcorder. \\tspam',\n",
       " 'Congrats! 1 year special cinema pass for 2 is yours. reply to this sms to claim your prize.\\tspam',\n",
       " 'I am pleased to tell you that you are awarded with a 1500 Bonus Prize, reply to this sms to claim your prize.\\tspam',\n",
       " 'Dont worry. I guess he is busy.\\tnot spam',\n",
       " 'Going for dinner. msg you later.\\tnot spam',\n",
       " 'Ok, I will call you up when I get some cash.\\tnot spam',\n",
       " '']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"data2.txt\",\"r\")\n",
    "# read the file \"data2.txt\" and split each\n",
    "# line in the file using newline \n",
    "data = file.read().split(\"\\n\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7cd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Congrats, You have won!! reply to our sms for a free nokia mobile + free camcorder. ',\n",
       "  'spam'],\n",
       " ['Congrats! 1 year special cinema pass for 2 is yours. reply to this sms to claim your prize.',\n",
       "  'spam'],\n",
       " ['I am pleased to tell you that you are awarded with a 1500 Bonus Prize, reply to this sms to claim your prize.',\n",
       "  'spam'],\n",
       " ['Dont worry. I guess he is busy.', 'not spam'],\n",
       " ['Going for dinner. msg you later.', 'not spam'],\n",
       " ['Ok, I will call you up when I get some cash.', 'not spam'],\n",
       " ['']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# futher split each line using tab\n",
    "data = [d.split(\"\\t\") for d in data]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af7069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Congrats, You have won!! reply to our sms for a free nokia mobile + free camcorder. ',\n",
       "  'spam'],\n",
       " ['Congrats! 1 year special cinema pass for 2 is yours. reply to this sms to claim your prize.',\n",
       "  'spam'],\n",
       " ['I am pleased to tell you that you are awarded with a 1500 Bonus Prize, reply to this sms to claim your prize.',\n",
       "  'spam'],\n",
       " ['Dont worry. I guess he is busy.', 'not spam'],\n",
       " ['Going for dinner. msg you later.', 'not spam'],\n",
       " ['Ok, I will call you up when I get some cash.', 'not spam']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the last list from the data\n",
    "del(data[-1])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330e461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Congrats, You have won!! reply to our sms for ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am pleased to tell you that you are awarded ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dont worry. I guess he is busy.</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Going for dinner. msg you later.</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ok, I will call you up when I get some cash.</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Congrats, You have won!! reply to our sms for ...      spam\n",
       "1  Congrats! 1 year special cinema pass for 2 is ...      spam\n",
       "2  I am pleased to tell you that you are awarded ...      spam\n",
       "3                    Dont worry. I guess he is busy.  not spam\n",
       "4                   Going for dinner. msg you later.  not spam\n",
       "5       Ok, I will call you up when I get some cash.  not spam"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data,columns = [\"text\",\"label\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats, You have won!! reply to our sms for a free nokia mobile + free camcorder. \n",
      "Congrats! 1 year special cinema pass for 2 is yours. reply to this sms to claim your prize.\n",
      "I am pleased to tell you that you are awarded with a 1500 Bonus Prize, reply to this sms to claim your prize.\n",
      "Dont worry. I guess he is busy.\n",
      "Going for dinner. msg you later.\n",
      "Ok, I will call you up when I get some cash.\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "text_vector = []\n",
    "\n",
    "for each in df[\"text\"]:\n",
    "    print(each)\n",
    "    # collect vocabularies (or words) from input \n",
    "    # datafile and store in a list called vocab\n",
    "    vocab.extend(each.lower().\\\n",
    "                 replace(\".\",\"\").\\\n",
    "                 replace(\",\",\"\").\\\n",
    "                 replace(\"+\",\"\").\\\n",
    "                 replace(\"!\",\"\").\\\n",
    "                 replace(\"1\",\"\").\\\n",
    "                 replace(\"1500\",\"\").\\\n",
    "                 replace(\"2\",\"\").\\\n",
    "                 split())\n",
    "    # collect vocabularies (or words) from input \n",
    "    # datafile, however store words in each line \n",
    "    # in a list called text_vector\n",
    "    text_vector.append(each.lower().\\\n",
    "                       replace(\".\",\"\").\\\n",
    "                       replace(\",\",\"\").\\\n",
    "                       replace(\"+\",\"\").\\\n",
    "                       replace(\"!\",\"\").\\\n",
    "                       replace(\"1\",\"\").\\\n",
    "                       replace(\"1500\",\"\").\\\n",
    "                       replace(\"2\",\"\").\\\n",
    "                       split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd42e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['congrats', 'you', 'have', 'won', 'reply', 'to', 'our', 'sms', 'for', 'a', 'free', 'nokia', 'mobile', 'free', 'camcorder', 'congrats', 'year', 'special', 'cinema', 'pass', 'for', 'is', 'yours', 'reply', 'to', 'this', 'sms', 'to', 'claim', 'your', 'prize', 'i', 'am', 'pleased', 'to', 'tell', 'you', 'that', 'you', 'are', 'awarded', 'with', 'a', '500', 'bonus', 'prize', 'reply', 'to', 'this', 'sms', 'to', 'claim', 'your', 'prize', 'dont', 'worry', 'i', 'guess', 'he', 'is', 'busy', 'going', 'for', 'dinner', 'msg', 'you', 'later', 'ok', 'i', 'will', 'call', 'you', 'up', 'when', 'i', 'get', 'some', 'cash']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c16f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb66bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['free', 'sms', 'msg', 'cash', 'pass', 'mobile', 'some', 'get', 'with', 'is', 'prize', 'worry', 'guess', 'awarded', 'he', 'that', 'are', 'cinema', 'a', 'call', 'i', 'busy', 'yours', 'reply', 'dont', 'tell', 'have', '500', 'when', 'dinner', 'going', 'congrats', 'year', 'your', 'will', 'ok', 'claim', 'up', 'camcorder', 'this', 'to', 'am', 'pleased', 'bonus', 'later', 'won', 'nokia', 'special', 'for', 'you', 'our']\n"
     ]
    }
   ],
   "source": [
    "# remove dublicates words \n",
    "vocab = list(set(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce691422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['congrats', 'you', 'have', 'won', 'reply', 'to', 'our', 'sms', 'for', 'a', 'free', 'nokia', 'mobile', 'free', 'camcorder'], ['congrats', 'year', 'special', 'cinema', 'pass', 'for', 'is', 'yours', 'reply', 'to', 'this', 'sms', 'to', 'claim', 'your', 'prize'], ['i', 'am', 'pleased', 'to', 'tell', 'you', 'that', 'you', 'are', 'awarded', 'with', 'a', '500', 'bonus', 'prize', 'reply', 'to', 'this', 'sms', 'to', 'claim', 'your', 'prize'], ['dont', 'worry', 'i', 'guess', 'he', 'is', 'busy'], ['going', 'for', 'dinner', 'msg', 'you', 'later'], ['ok', 'i', 'will', 'call', 'you', 'up', 'when', 'i', 'get', 'some', 'cash']]\n"
     ]
    }
   ],
   "source": [
    "# display words in each line from the input file\n",
    "print(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c08d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of list containing number of words presented in each sentence or line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_num = []\n",
    "for text in text_vector:\n",
    "    vector = []\n",
    "    # count number of words presented in vocabulary for each line\n",
    "    for word in vocab:\n",
    "        vector.append(text.count(word))\n",
    "    text_vector_num.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb6992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_vector_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(text_vector_num[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fa7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Display new representation of each sentence in the dataset\n",
    "for each in text_vector_num:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214aabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['free', 'sms', 'msg', 'cash', 'pass', 'mobile', 'some', 'get', 'with', 'is', 'prize', 'worry', 'guess', 'awarded', 'he', 'that', 'are', 'cinema', 'a', 'call', 'i', 'busy', 'yours', 'reply', 'dont', 'tell', 'have', '500', 'when', 'dinner', 'going', 'congrats', 'year', 'your', 'will', 'ok', 'claim', 'up', 'camcorder', 'this', 'to', 'am', 'pleased', 'bonus', 'later', 'won', 'nokia', 'special', 'for', 'you', 'our'], [2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1], [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0], [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# insert vocabulary list at index 0\n",
    "text_vector_num.insert(0, vocab)\n",
    "print(text_vector_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display numerical vectors correspondind to spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687dc57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam message vectors \n",
      " [[2 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0\n",
      "  0 0 1 0 1 0 0 0 0 1 1 0 1 1 1]\n",
      " [0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0\n",
      "  1 0 0 1 2 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 1 0 2 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
      "  1 0 0 1 3 1 1 1 0 0 0 0 0 2 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get spam message vector\n",
    "import numpy as np\n",
    "spam = np.array(text_vector_num[1:4])\n",
    "print('Spam message vectors \\n', spam)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b555ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display numerical vectors correspondind to non-spam messages¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e78df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham message vectors \n",
      " [[0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 1 1 0]\n",
      " [0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# get non-span message vector\n",
    "ham = np.array(text_vector_num[4:])\n",
    "print('Ham message vectors \\n', ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f45167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute  𝑝(𝑤𝑘│𝑠𝑝𝑎𝑚)\n",
    "# Probability that a given word ( 𝑤𝑘\n",
    "#   or feature) comes from spam messages\n",
    "\n",
    "# Number of times a particular word occur in spam messages\n",
    "# 𝑛𝑘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0001821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 0, 0, 1, 1, 0, 0, 1, 1, 3, 0, 0, 1, 0, 1, 1, 1, 2, 0, 1, 0,\n",
       "       1, 3, 0, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0, 0, 2, 0, 1, 2, 6, 1, 1, 1,\n",
       "       0, 1, 1, 1, 2, 3, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk_spam = np.sum(spam,axis=0) \n",
    "nk_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5fc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words in spam messages\n",
    "# 𝑛𝑠𝑝𝑎𝑚\n",
    "n_spam = np.sum(spam) \n",
    "n_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02857143, 0.03809524, 0.00952381, 0.00952381, 0.01904762,\n",
       "       0.01904762, 0.00952381, 0.00952381, 0.01904762, 0.01904762,\n",
       "       0.03809524, 0.00952381, 0.00952381, 0.01904762, 0.00952381,\n",
       "       0.01904762, 0.01904762, 0.01904762, 0.02857143, 0.00952381,\n",
       "       0.01904762, 0.00952381, 0.01904762, 0.03809524, 0.00952381,\n",
       "       0.01904762, 0.01904762, 0.01904762, 0.00952381, 0.00952381,\n",
       "       0.00952381, 0.02857143, 0.01904762, 0.02857143, 0.00952381,\n",
       "       0.00952381, 0.02857143, 0.00952381, 0.01904762, 0.02857143,\n",
       "       0.06666667, 0.01904762, 0.01904762, 0.01904762, 0.00952381,\n",
       "       0.01904762, 0.01904762, 0.01904762, 0.02857143, 0.03809524,\n",
       "       0.01904762])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_wk_given_spam_message = (nk_spam + 1) / (n_spam + len(vocab))\n",
    "prob_of_wk_given_spam_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df747b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute  𝑝(𝑤𝑘│ℎ𝑎𝑚)\n",
    " \n",
    "# Probability that a given word ( 𝑤𝑘\n",
    "#   or feature) comes from ham messages\n",
    "\n",
    "# Number of times a particular word occur in ham messages\n",
    "# 𝑛𝑘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49965cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 3, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk_ham = np.sum(ham,axis=0) \n",
    "nk_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f995d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in ham messages\n",
    "# 𝑛ℎ𝑎𝑚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5f4846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ham = np.sum(ham) \n",
    "n_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01333333, 0.01333333, 0.02666667, 0.02666667, 0.01333333,\n",
       "       0.01333333, 0.02666667, 0.02666667, 0.01333333, 0.02666667,\n",
       "       0.01333333, 0.02666667, 0.02666667, 0.01333333, 0.02666667,\n",
       "       0.01333333, 0.01333333, 0.01333333, 0.01333333, 0.02666667,\n",
       "       0.05333333, 0.02666667, 0.01333333, 0.01333333, 0.02666667,\n",
       "       0.01333333, 0.01333333, 0.01333333, 0.02666667, 0.02666667,\n",
       "       0.02666667, 0.01333333, 0.01333333, 0.01333333, 0.02666667,\n",
       "       0.02666667, 0.01333333, 0.02666667, 0.01333333, 0.01333333,\n",
       "       0.01333333, 0.01333333, 0.01333333, 0.01333333, 0.02666667,\n",
       "       0.01333333, 0.01333333, 0.01333333, 0.02666667, 0.04      ,\n",
       "       0.01333333])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_wk_given_ham_message = (nk_ham + 1) / (n_ham + len(vocab))\n",
    "prob_of_wk_given_ham_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilities of a message being spam and ham\n",
    "# There are six messages in the file. Out of six messages, 3 are spam and rest of the messages are ham or non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_message_is_spam,prob_message_is_ham = 3/6,3/6\n",
    "prob_message_is_spam,prob_message_is_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f8c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af2259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new sample messages and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dc96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am', 'busy', 'i', 'will', 'msg', 'you', 'later'],\n",
       " ['congrats', 'you', 'are', 'awarded', 'a', 'free', 'mobile']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = [\"I am busy. I will msg you later.\",\"Congrats! You are awarded a free mobile.\"]\n",
    "new_words= []\n",
    "for each in new:\n",
    "    processed = each.lower().replace(\".\",\"\").replace(\",\",\"\").replace(\"+\",\"\").replace(\"!\",\"\").split()\n",
    "    new_words.append(processed)\n",
    "new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict(message_list,prob_spam,prob_ham,prob_of_feature_given_spam_message,prob_of_feature_given_ham_message,vocab):\n",
    "    \"\"\"\n",
    "    Returns whether or not the given message (a list of words) is spam or ham\n",
    "    \n",
    "    \n",
    "    Input Parameters\n",
    "    ---------\n",
    "    \n",
    "    message_list: a list of words\n",
    "    \n",
    "    prob_spam: a float\n",
    "    \n",
    "    prob_ham: a float\n",
    "    \n",
    "    prob_of_feature_given_spam_message: an numpy array\n",
    "    \n",
    "    prob_of_feature_given_ham_message: an numpy array\n",
    "    \n",
    "    vocab: a list of words    \n",
    "             \n",
    "    \n",
    "    \"\"\"\n",
    "    spam = prob_spam\n",
    "    ham = prob_ham\n",
    "    # compute 𝑝(𝑦) * ∏(𝑗=1 to 𝑑) 𝑝(𝑎_𝑗│𝑦)\n",
    "    \n",
    "    for word in message_list:\n",
    "        spam *= prob_of_feature_given_spam_message[vocab.index(word)]\n",
    "        ham *= prob_of_feature_given_ham_message[vocab.index(word)]\n",
    "     \n",
    "    print('-----------------------------------------')\n",
    "    print('The given message: ', \" \".join(message_list))\n",
    "    print()\n",
    "    print('Prob that the given message is spam = ',spam)\n",
    "    print()\n",
    "    print('Prob that the given message is ham = ',ham)\n",
    "    print('------------------------------------------')\n",
    "    \n",
    "    \n",
    "    if spam > ham:\n",
    "        return \" \".join(message_list),\"spam\"\n",
    "    else:\n",
    "        return \" \".join(message_list),\"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bbddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "The given message:  i am busy i will msg you later\n",
      "\n",
      "Prob that the given message is spam =  1.0829429792459005e-15\n",
      "\n",
      "Prob that the given message is ham =  3.835668952903523e-13\n",
      "------------------------------------------\n",
      "Result:  ham\n"
     ]
    }
   ],
   "source": [
    "result = predict(new_words[0],prob_message_is_spam,prob_message_is_ham,prob_of_wk_given_spam_message,prob_of_wk_given_ham_message,vocab)\n",
    "print('Result: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf5eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "The given message:  congrats you are awarded a free mobile\n",
      "\n",
      "Prob that the given message is spam =  3.0701433461621255e-12\n",
      "\n",
      "Prob that the given message is ham =  1.1237311385459537e-13\n",
      "------------------------------------------\n",
      "Result:  spam\n"
     ]
    }
   ],
   "source": [
    "result = predict(new_words[1],prob_message_is_spam,prob_message_is_ham,prob_of_wk_given_spam_message,prob_of_wk_given_ham_message,vocab)\n",
    "print('Result: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5b7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f10aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ab6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f37e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
